{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkj9BIFPy7h6"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from io import StringIO\n",
        "import re\n",
        "\n",
        "# === Configuraci√≥n de S3 ===\n",
        "S3_BUCKET = \"headlinesjobs\"\n",
        "RAW_PREFIX = \"headlines/raw/\"\n",
        "FINAL_PREFIX = \"headlines/final/periodico=eltiempo\"\n",
        "s3_client = boto3.client(\"s3\")\n",
        "\n",
        "def extract_data_from_html(html_content):\n",
        "    \"\"\"Extrae categor√≠a, titular y enlace de art√≠culos de eltiempo.com\"\"\"\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    news_data = []\n",
        "\n",
        "    for article in soup.find_all(\"article\", class_=\"c-articulo--textual\"):\n",
        "        title_tag = article.find(\"a\", class_=\"c-articulo__titulo__txt\")\n",
        "        if not title_tag:\n",
        "            continue\n",
        "        title = title_tag.get_text(strip=True)\n",
        "        link = title_tag.get(\"href\", \"Sin enlace\")\n",
        "        category = article.get(\"data-category\", \"Sin categor√≠a\")\n",
        "        news_data.append([category, title, link])\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def main():\n",
        "    # Listar todos los archivos HTML bajo headlines/raw/\n",
        "    response = s3_client.list_objects_v2(Bucket=S3_BUCKET, Prefix=RAW_PREFIX)\n",
        "\n",
        "    if \"Contents\" not in response:\n",
        "        print(\"‚ùå No se encontraron archivos en el bucket.\")\n",
        "        return\n",
        "\n",
        "    for obj in response[\"Contents\"]:\n",
        "        s3_key = obj[\"Key\"]\n",
        "        if not s3_key.endswith(\".html\"):\n",
        "            continue\n",
        "\n",
        "        print(f\"üìÑ Procesando archivo: s3://{S3_BUCKET}/{s3_key}\")\n",
        "        html_obj = s3_client.get_object(Bucket=S3_BUCKET, Key=s3_key)\n",
        "        html_content = html_obj[\"Body\"].read()\n",
        "\n",
        "        news = extract_data_from_html(html_content)\n",
        "        if not news:\n",
        "            print(f\"‚ö†Ô∏è No se encontraron noticias en {s3_key}\")\n",
        "            continue\n",
        "\n",
        "        df = pd.DataFrame(news, columns=[\"Categoria\", \"Titular\", \"Enlace\"])\n",
        "\n",
        "        # Extraer la fecha desde el nombre del archivo\n",
        "        match = re.search(r\"contenido-(\\d{4})-(\\d{2})-(\\d{2})\\.html\", s3_key)\n",
        "        if match:\n",
        "            year, month, day = match.groups()\n",
        "        else:\n",
        "            # Si no se puede extraer, usar fecha actual\n",
        "            now = datetime.utcnow()\n",
        "            year, month, day = now.strftime(\"%Y\"), now.strftime(\"%m\"), now.strftime(\"%d\")\n",
        "\n",
        "        output_key = f\"{FINAL_PREFIX}/year={year}/month={month}/day={day}/headlines.csv\"\n",
        "        csv_buffer = StringIO()\n",
        "        df.to_csv(csv_buffer, index=False)\n",
        "\n",
        "        s3_client.put_object(\n",
        "            Bucket=S3_BUCKET,\n",
        "            Key=output_key,\n",
        "            Body=csv_buffer.getvalue()\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ CSV creado en s3://{S3_BUCKET}/{output_key}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}